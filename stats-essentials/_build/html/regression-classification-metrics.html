

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Regression, Classification, Evaluation &mdash; Statistics Essentials 1.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Helpful math" href="helpful-math.html" />
    <link rel="prev" title="Statistical Inference" href="statistical-inference.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> Statistics Essentials
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Statistics:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro-stats.html">Statistics Essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-concepts.html">Probability Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="combinatorics.html">Combinatorics</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability.html">Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability-distributions.html">Probability Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="paradigms.html">Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="statistics-concepts.html">Statistics Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="statistical-inference.html">Statistical Inference</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Regression, Classification, Evaluation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#predictive-modeling">Predictive Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#types-of-data">Types of Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#supervised-learning">Supervised Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#data-size">Data Size</a></li>
<li class="toctree-l3"><a class="reference internal" href="#unupervised-learning">Unupervised Learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#linear-regression">Linear Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#assumptions">Assumptions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-metrics">Evaluation Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#further-study">Further Study</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Appendices:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="helpful-math.html">Helpful math</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-python.html">Python Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">Works cited</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Statistics Essentials</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Regression, Classification, Evaluation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/regression-classification-metrics.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="regression-classification-evaluation">
<h1>Regression, Classification, Evaluation<a class="headerlink" href="#regression-classification-evaluation" title="Permalink to this headline">¶</a></h1>
<p><strong>Objectives</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><p>State assumptions of linear regression model</p></li>
<li><p>Estimate a linear regression model</p></li>
<li><p>Evaluate a linear regression model</p></li>
</ol>
</div></blockquote>
<div class="section" id="predictive-modeling">
<h2>Predictive Modeling<a class="headerlink" href="#predictive-modeling" title="Permalink to this headline">¶</a></h2>
<p>There are many circumstances in which we are interested in predicting some outcome <span class="math notranslate nohighlight">\(Y\)</span>.
To accomplish this task we set about collecting, selecting, and constructing data
– a process referred to as <strong>feature engineering</strong> – that we think would help predict <span class="math notranslate nohighlight">\(Y\)</span>.
Given our selected features <span class="math notranslate nohighlight">\(\textbf{X}\)</span>, the association between the
features <span class="math notranslate nohighlight">\(\textbf{X}\)</span> and and the outcome <span class="math notranslate nohighlight">\(Y\)</span> can be expressed as</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[Y = f(\mathbf{X}) + \epsilon\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(f\)</span> explicitly describes the precise relationship between
<span class="math notranslate nohighlight">\(\textbf{X}\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, and <span class="math notranslate nohighlight">\(\epsilon\)</span> is…</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>CLASS DISCUSSION</strong></p>
<ul class="simple">
<li><p>What is <span class="math notranslate nohighlight">\(\epsilon\)</span>?</p></li>
<li><p>How could we model <span class="math notranslate nohighlight">\(\epsilon\)</span>?</p></li>
<li><p>How about <span class="math notranslate nohighlight">\(f\)</span>?</p></li>
<li><p>Based on what you’ve seen so far, is <em>Statistics and Probability</em> more suited to describe <span class="math notranslate nohighlight">\(\epsilon\)</span> or <span class="math notranslate nohighlight">\(f\)</span>?</p></li>
</ul>
</div>
</div>
<div class="section" id="types-of-data">
<h2>Types of Data<a class="headerlink" href="#types-of-data" title="Permalink to this headline">¶</a></h2>
<p>The data <span class="math notranslate nohighlight">\(\textbf{X}\)</span> that we use to predict <span class="math notranslate nohighlight">\(Y\)</span> can come in many varieties, i.e.</p>
<ol class="arabic simple">
<li><p><strong>Continuous</strong>:</p>
<ul class="simple">
<li><p>Price, Quantity, Sales, Tenure</p></li>
<li><p>Sometimes it makes sense to map to discrete variables</p></li>
</ul>
</li>
<li><p><strong>Categorical</strong>:</p>
<ul class="simple">
<li><p>Yes/No, 0/1, Treated/Control, High/Medium/Low</p></li>
<li><p>Also called a factor</p></li>
</ul>
</li>
<li><p><strong>Missing Values</strong></p>
<ul class="simple">
<li><p>May require estimation</p></li>
</ul>
</li>
<li><p><strong>“Non-numeric”</strong></p>
<ul class="simple">
<li><p>Text, Audio, Images, Signals, Graphs</p></li>
<li><p>Requires transformation into meaningful quantitative features</p></li>
</ul>
</li>
</ol>
</div>
<div class="section" id="supervised-learning">
<h2>Supervised Learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">¶</a></h2>
<p>Approximating <span class="math notranslate nohighlight">\(f(\textbf{X})\)</span> with some function <span class="math notranslate nohighlight">\(\hat{f}(\textbf{X})\)</span>
in the face of noisy data (as a result of <span class="math notranslate nohighlight">\(\epsilon\)</span>)
is known as <strong>model fitting</strong>. Actually, different disciplines have adopted
different nomenclatures for this process.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 17%" />
<col style="width: 34%" />
<col style="width: 49%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Machine Learning</p></th>
<th class="head"><p>Notation</p></th>
<th class="head"><p>Other Fields</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Features</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\textbf{X}_{(n \times p)}\)</span></p></td>
<td><p>Covariates, Independent Variables, or Regressors</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Targets</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(Y_{(n \times 1)}\)</span></p></td>
<td><p>Outcome, Dependent or Endogenous Variable</p></td>
</tr>
<tr class="row-even"><td><p><strong>Training</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\hat{f}\)</span></p></td>
<td><p>Learning, Estimation, or Model Fitting</p></td>
</tr>
</tbody>
</table>
<p>Depending on the data type of the target,
the <em>supervised learning</em> problem is referred to as either</p>
<blockquote>
<div><ul>
<li><p><strong>Regression</strong> (when <span class="math notranslate nohighlight">\(Y\)</span> is real-valued)</p>
<blockquote>
<div><p>E.g., if you are predicting price, demand, or size.</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<blockquote>
<div><ul>
<li><p><strong>Classification</strong> (when <span class="math notranslate nohighlight">\(Y\)</span> is categorical)</p>
<blockquote>
<div><p>E.g., if you are prediction fraud or churn</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<div class="section" id="data-size">
<h3>Data Size<a class="headerlink" href="#data-size" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><ul class="simple">
<li><p><strong>Associative Studies</strong>: test hypotheses</p>
<ul>
<li><p>Association <em>is</em> causality under carefully controlled conditions</p></li>
<li><p>The <strong>power</strong> and accuracy of a test is an asymptotic function of <span class="math notranslate nohighlight">\(N\)</span></p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong>Predictive Studies</strong>: try to guess well</p>
<ul>
<li><p>Complex models are prone to <strong>overfitting</strong> without sufficient <span class="math notranslate nohighlight">\(N\)</span></p></li>
<li><p><strong>Regularization</strong> limits <em>overfitting</em> and <strong>cross-validation</strong> assesses <em>accuracy</em></p></li>
</ul>
</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>CLASS DISCUSSION</strong></p>
<p>How would you differentiate <em>Statistics</em> and <em>Machine Learning</em>, if at all?</p>
</div>
</div>
<div class="section" id="unupervised-learning">
<h3>Unupervised Learning<a class="headerlink" href="#unupervised-learning" title="Permalink to this headline">¶</a></h3>
<p>When you’re not trying to predict a target <span class="math notranslate nohighlight">\(Y\)</span>,
but just seeking to uncover patterns and structures
between the features <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, the problem is referred to
as <strong>Unsupervised Learning</strong>. The two primary areas of unsupervised
learning are</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>Clustering</strong>: e.g., hierarchical, k-means</p></li>
<li><p><strong>Dimension reduction</strong>: e.g., PCA, SVD, NMF</p></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="linear-regression">
<h2>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h2>
<p>Suppose that <span class="math notranslate nohighlight">\(Y_i\)</span> depends on <span class="math notranslate nohighlight">\(X_i\)</span> according to</p>
<div class="math notranslate nohighlight">
\[Y_i = \beta_{0} + \beta_{1} X_i + \epsilon_i, \text{ where } \epsilon_i \overset{\small i.i.d.}{\sim}N\left(0, \sigma^2\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta_{0}\)</span>, <span class="math notranslate nohighlight">\(\beta_{1}\)</span>, and <span class="math notranslate nohighlight">\(\sigma^2\)</span> are the parameters of the model
(intercept, coefficient and variance, respectively).</p>
<p>We can easily simulate some data under an instance of this set of models as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">get_simple_regression_samples</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">b0</span><span class="o">=-</span><span class="mf">0.3</span><span class="p">,</span><span class="n">b1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">error</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="n">trueX</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
    <span class="n">trueT</span> <span class="o">=</span> <span class="n">b0</span> <span class="o">+</span> <span class="p">(</span><span class="n">b1</span><span class="o">*</span><span class="n">trueX</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">trueX</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">trueT</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">error</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">b0_true</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.3</span>
<span class="n">b1_true</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">get_simple_regression_samples</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">b0</span><span class="o">=</span><span class="n">b0_true</span><span class="p">,</span><span class="n">b1</span><span class="o">=</span><span class="n">b1_true</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">y</span><span class="p">,</span><span class="s1">&#39;ko&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">b0_true</span> <span class="o">+</span> <span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">b1_true</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;model mean&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>(<a class="reference external" href="./scripts/linear-regression.py">Source code</a>, <a class="reference external" href="./scripts/linear-regression_00_00.png">png</a>, <a class="reference external" href="./scripts/linear-regression_00_00.hires.png">hires.png</a>, <a class="reference external" href="./scripts/linear-regression_00_00.pdf">pdf</a>)</p>
<div class="figure align-default">
<img alt="_images/linear-regression_00_00.png" src="_images/linear-regression_00_00.png" />
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>QUESTION</strong></p>
<p>If you added data into the plot above where could you add them that might be a cause for concern?</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>CLASS DISCUSSION</strong></p>
<p>If you increased to total number of data points generated by this model, how would the density of points in this picture look?</p>
</div>
<p>Now of course in real life you <em>first</em> get your data and <em>then</em> you estimate your model:</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} = \mathbf{X}\mathbf{\hat \beta} + \mathbf{\hat \epsilon}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{y} = \left[\begin{array}{c}y_1\\y_2\\\vdots\\y_n\end{array}\right]_, \;\;\mathbf{X} = \left[\begin{array}{c}1&amp;x_1\\1&amp;x_2\\\vdots\\1&amp;x_n\end{array}\right]_,  \;\;  \mathbf{\hat \beta} = \left[\begin{array}{c} \hat \beta_0\\ \hat \beta_1 \end{array}\right]\text{ and } \mathbf{\hat \epsilon} = \left[\begin{array}{c}\hat \epsilon_1\\\hat \epsilon_2\\\vdots\\ \hat \epsilon_n\end{array}\right]\)</span></p>
<p>and the predictions from the model are</p>
<div class="math notranslate nohighlight">
\[\mathbf{\hat Y_0} = \mathbf{X_0}\mathbf{\hat \beta}\]</div>
<p>The <strong>residuals</strong> <span class="math notranslate nohighlight">\(\hat \epsilon_i\)</span> are used to estimate the model <strong>mean squared error (MSE)</strong></p>
<div class="math notranslate nohighlight">
\[\displaystyle \frac{n-p-1}{n} \hat \sigma^2 = \sum_{i=1}^n \frac{\epsilon_i^2}{n}\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> is the number of <em>coefficients</em> in the model (here, 1).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span>

<span class="k">def</span> <span class="nf">fit_linear_lstsq</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span><span class="n">ydata</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    y = b0 + b1*x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">matrix</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n</span><span class="p">,</span><span class="n">d</span> <span class="o">=</span> <span class="n">xdata</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">matrix</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">xdata</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>
    <span class="k">return</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">basic</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span><span class="n">ydata</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">coefs_lstsq</span> <span class="o">=</span> <span class="n">fit_linear_lstsq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_lstsq</span> <span class="o">=</span> <span class="n">coefs_lstsq</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">coefs_lstsq</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;truth: b0=</span><span class="si">%s</span><span class="s2">,b1=</span><span class="si">%s</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">b0_true</span><span class="p">,</span><span class="n">b1_true</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;lstsq fit: b0=</span><span class="si">%s</span><span class="s2">,b1=</span><span class="si">%s</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">coefs_lstsq</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">3</span><span class="p">),</span><span class="nb">round</span><span class="p">(</span><span class="n">coefs_lstsq</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>EXERCISE</strong></p>
<p>Try out the above code.  If it’s making sense to you, try seeing what happens when you change the sample size <span class="math notranslate nohighlight">\(n\)</span>,
or the model intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> and coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span> used to generate the sample.
See if you are able to add the model fit line to the plot of the actual model line itself (from the plot above).</p>
</div>
<div class="section" id="assumptions">
<h3>Assumptions<a class="headerlink" href="#assumptions" title="Permalink to this headline">¶</a></h3>
<p>The specification here actually entails many assumptions:</p>
<ol class="arabic">
<li><p><strong>Fixed and Constant</strong> <span class="math notranslate nohighlight">\(\mathbf{X}\)</span></p>
<p>The <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> are assumed to be measured exactly without error</p>
</li>
</ol>
<ol class="arabic" start="2">
<li><p><strong>Independent Errors/Outcomes</strong> <span class="math notranslate nohighlight">\(\epsilon/Y\)</span></p>
<p>The final value for any <span class="math notranslate nohighlight">\(Y_i\)</span> (or equivalently, <span class="math notranslate nohighlight">\(\epsilon_i\)</span>) can not be
dependent on any other <span class="math notranslate nohighlight">\(Y_j\)</span> or <span class="math notranslate nohighlight">\(\epsilon_j\)</span>, <span class="math notranslate nohighlight">\(j \not = i\)</span></p>
</li>
</ol>
<ol class="arabic" start="3">
<li><p><strong>Linear Model Form</strong></p>
<p>The linear relationships as specified by the model are correct.
This is equivalent to having <strong>Unbiased Errors</strong>. I.e., the expected value of the error
<span class="math notranslate nohighlight">\(\epsilon_i\)</span> is 0 for all levels of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p>While only linear forms are allowed, the forms are only linear in the model coefficients (not the features).
I.e., any features (e.g., non-linear functions of features like polynomials or spline basis functions)
are permissible.</p>
</li>
</ol>
<ol class="arabic" start="4">
<li><p><strong>Normal Errors</strong></p>
<p>The errors <span class="math notranslate nohighlight">\(\epsilon_i\)</span> around <span class="math notranslate nohighlight">\(\mathbf{X}\beta\)</span> are normally distributed</p>
</li>
</ol>
<ol class="arabic" start="5">
<li><p><strong>Homoscedastic Errors</strong></p>
<p>The errors <span class="math notranslate nohighlight">\(\epsilon_i\)</span> have constant variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span>, for all levels of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
</li>
</ol>
<ol class="upperalpha" start="24">
<li><p><strong>Full Rank of</strong> <span class="math notranslate nohighlight">\(X\)</span></p>
<p>The features are not “redundant”; and, being nearly so hurts model performance.</p>
</li>
</ol>
<p>Fortunately, this model can still be effective when some of the assumptions
do not fully hold.  In addition, there are methods available to help address
and correct failures of the assumptions.</p>
<p>Assumptions play a major statistical inference problems (i.e., association studies),
but are less relevant in prediction contexts where it doesn’t matter how or why it works –
just whether or not it does. As a result, <em>machine learning</em>
has been able to produce creative and powerful alternatives to the
<em>linear regression model</em> shown above. E.g., k-nearest neighbors, random forests,
gradient boosting, support vector machines, and neural networks.</p>
</div>
</div>
<div class="section" id="evaluation-metrics">
<h2>Evaluation Metrics<a class="headerlink" href="#evaluation-metrics" title="Permalink to this headline">¶</a></h2>
<p><strong>Regression</strong></p>
<p>In <em>regression</em> contexts the fit of the model to the data can be assessed using the <em>MSE</em>, from above,
or the <strong>root mean squared error (RMSE)</strong></p>
<div class="math notranslate nohighlight">
\[\displaystyle \sqrt{\sum_{i=1}^n \frac{(y_i-\hat y_i)^2}{n}}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>EXERCISE</strong></p>
<p>Calculate the RMSE for the data and prediction in the code above.</p>
</div>
<p><strong>Classification</strong></p>
<p>In <em>classification</em> contexts, performance is assessed using a <strong>confusion matrix</strong>:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 35%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>Predicted False <span class="math notranslate nohighlight">\((\hat Y = 0)\)</span></p></th>
<th class="head"><p>Predicted True <span class="math notranslate nohighlight">\((\hat Y = 1)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>True <span class="math notranslate nohighlight">\((Y = 0)\)</span></p></td>
<td><p>True Negatives <span class="math notranslate nohighlight">\((TN)\)</span></p></td>
<td><p>False Positive <span class="math notranslate nohighlight">\((FP)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>True <span class="math notranslate nohighlight">\((Y = 1)\)</span></p></td>
<td><p>False Negatives <span class="math notranslate nohighlight">\((FN)\)</span></p></td>
<td><p>True Positives <span class="math notranslate nohighlight">\((TP)\)</span></p></td>
</tr>
</tbody>
</table>
<p>There are many ways to evaluate the confusion matrix:</p>
<blockquote>
<div><ul class="simple">
<li><p>Accuracy = <span class="math notranslate nohighlight">\(\frac{TN+TP}{FP+FP+TN+TP}\)</span>: overall proportion correct</p></li>
</ul>
</div></blockquote>
<blockquote>
<div><ul class="simple">
<li><p>Precision = <span class="math notranslate nohighlight">\(\frac{TP}{TP+FP}\)</span>: proportion called true that are correct</p></li>
</ul>
</div></blockquote>
<blockquote>
<div><ul class="simple">
<li><p>Recall =  <span class="math notranslate nohighlight">\(\frac{TP}{TP+FN}\)</span>: proportion of true that are called correctly</p></li>
</ul>
</div></blockquote>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F_1\)</span>-Score = <span class="math notranslate nohighlight">\(\frac{2}{ \frac{1}{recall} + \frac{1}{precision}  }\)</span>: balancing Precision/Recall</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="further-study">
<h2>Further Study<a class="headerlink" href="#further-study" title="Permalink to this headline">¶</a></h2>
<p>A good place to start a review of the content here is:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=LvaTokhYnDw&amp;list=PL5-da3qGB5ICcUhueCyu25slvsGp8IDTa">Hastie and Rob Tibshirani (Supervised and Unsupervised learning)</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=WjyuiK5taS8&amp;list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy">Hastie and Rob Tibshirani (Linear Regression)</a></p></li>
</ul>
<div class="admonition-check-for-understanding admonition">
<p class="admonition-title">Check for understanding</p>
<p>Which of the following areas of machine learning contain the mentioned categories.</p>
<div class="toggle docutils container">
<div class="header docutils container">
<ul class="simple">
<li><p><strong>(A)</strong>: Supervised learning &lt;- classification, regression</p></li>
<li><p><strong>(B)</strong>: Supervised learning &lt;- regression, clustering</p></li>
<li><p><strong>(C)</strong>: Unsupervised learning &lt;- dimension reduction, classification</p></li>
<li><p><strong>(D)</strong>: Unsupervised learning &lt;- dimension reduction, clustering</p></li>
<li><p><strong>(E)</strong>: Unsupervised learning &lt;- clustering, classification</p></li>
</ul>
</div>
<p><strong>ANSWER</strong>:</p>
<blockquote>
<div><p><strong>(A)</strong> and <strong>(D)</strong></p>
</div></blockquote>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="helpful-math.html" class="btn btn-neutral float-right" title="Helpful math" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="statistical-inference.html" class="btn btn-neutral float-left" title="Statistical Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2019, Galvanize, Inc

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
  
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>